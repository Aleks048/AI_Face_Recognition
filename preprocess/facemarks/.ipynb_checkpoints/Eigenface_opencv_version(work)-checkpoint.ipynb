{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aaron_Eckhart', 'Aaron_Guiel', 'Aaron_Patterson', 'Aaron_Peirsol', 'Aaron_Pena', 'Aaron_Sorkin', 'Aaron_Tippin', 'Abbas_Kiarostami', 'Abba_Eban', 'Abdel_Aziz_Al-Hakim', 'Abdel_Madi_Shabneh', 'Abdel_Nasser_Assidi', 'Abdoulaye_Wade', 'Abdulaziz_Kamilov', 'Abdullah', 'Abdullah_Ahmad_Badawi', 'Abdullah_al-Attiyah', 'Abdullah_Gul', 'Abdullah_Nasseef', 'Abdullatif_Sener', 'Abdul_Majeed_Shobokshi', 'Abdul_Rahman', 'Abel_Aguilar', 'Abel_Pacheco', 'Abid_Hamid_Mahmud_Al-Tikriti', 'Abner_Martinez', 'Abraham_Foxman', 'Aby_Har-Even', 'Adam_Ant', 'Adam_Freier', 'Adam_Herbert', 'Adam_Kennedy', 'Adam_Mair', 'Adam_Rich', 'Adam_Sandler', 'Adam_Scott', 'Adelina_Avila', 'Adel_Al-Jubeir', 'Adisai_Bodharamik', 'Adolfo_Aguilar_Zinser', 'Adolfo_Rodriguez_Saa', 'Adoor_Gopalakarishnan', 'thyx']\n",
      "training is done\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6346.39\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6798.36\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6009.86\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6017.28\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5960.93\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5849.68\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6551.39\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6102.72\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6199.49\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6142.47\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6319.15\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6461.14\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6281.22\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7765.38\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5859.45\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:8041.83\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6343.81\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6258.62\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7125.06\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6031.57\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4963.91\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4460.21\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6275.92\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6081.77\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5365.66\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4833.81\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4419.42\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4172.29\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4222.73\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4472.18\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4921.28\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4969.35\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5561.27\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7336.07\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7380.90\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:8091.52\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:8931.93\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:9624.88\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:10099.99\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:9536.77\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:8714.11\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7695.73\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7573.65\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6746.86\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:42,confidence:6015.95\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:3,confidence:17381.81\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:42,confidence:6938.12\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:16,confidence:17864.60\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:42,confidence:6303.72\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:16,confidence:17960.89\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:42,confidence:6036.69\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:3,confidence:17984.70\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6594.03\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:42,confidence:6280.24\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:16,confidence:17942.54\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:42,confidence:6095.81\n",
      "lendmark:  2\n",
      "person[0]:  68\n",
      "person[0]:  68\n",
      "Label:3,confidence:17808.24\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6480.25\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6292.33\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5600.28\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5655.83\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5756.00\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5386.20\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5316.24\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:4955.17\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5551.33\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5440.22\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6113.92\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7135.57\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7233.33\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7947.50\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7202.85\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6894.75\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:8054.27\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7938.73\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5247.59\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5900.05\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6039.80\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6758.27\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6222.87\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5739.41\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5263.98\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5928.54\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5914.65\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:5179.91\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:6067.72\n",
      "lendmark:  1\n",
      "person[0]:  68\n",
      "Label:42,confidence:7248.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4bedbbdbf56b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'small_face_dataset'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;31m#     generator(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[0mFaceRec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-4bedbbdbf56b>\u001b[0m in \u001b[0;36mFaceRec\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dynamic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;31m#press q to exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xff\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mcamera\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "  \n",
    "#generate face data\n",
    "def generator(data):\n",
    "    name = input('my name:')\n",
    "    #if the directory exists, then delete\n",
    "    path = os.path.join(data,name)\n",
    "    if os.path.isdir(path):\n",
    "        shutil.rmtree(path) \n",
    "        \n",
    "    #create a directory\n",
    "    os.mkdir(path)\n",
    "    \n",
    "    #create a cascade classifier, load a classifier\n",
    "    face_cascade = cv2.CascadeClassifier('D:\\Python37\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    #open camera  \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow('Camera')\n",
    "    #the index of face images\n",
    "    count = 1\n",
    "    \n",
    "    while(True):\n",
    "        #read a frame\n",
    "        ret,frame = camera.read()\n",
    "        #if successful\n",
    "        if ret:\n",
    "            #grayscale\n",
    "            grayscale_image = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            #detect faces\n",
    "            faces = face_cascade.detectMultiScale(grayscale_image,1.3,5)            \n",
    "            for (x,y,w,h) in faces:\n",
    "                #draw rectangles \n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                #adjust the size of image, same as images in database\n",
    "                f = cv2.resize(frame[y:y+h,x:x+w],(250,250))\n",
    "                #save faces\n",
    "                cv2.imwrite('%s/%s.jpg'%(path,str(count)),f)\n",
    "                count += 1                \n",
    "            cv2.imshow('Camera',frame)            \n",
    "            #press q to exit\n",
    "            if cv2.waitKey(100) & 0xff == ord('q') :\n",
    "                break\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "def LoadImages(data):\n",
    "    images = []\n",
    "    labels = []\n",
    "    names = []\n",
    "    \n",
    "    label = 0\n",
    "    #read all face images from directory\n",
    "    for subDirname in os.listdir(data):\n",
    "        subjectPath = os.path.join(data,subDirname)\n",
    "        if os.path.isdir(subjectPath):                  \n",
    "            names.append(subDirname)\n",
    "            for fileName in os.listdir(subjectPath):\n",
    "                imgPath = os.path.join(subjectPath,fileName)\n",
    "                #convert into grascale\n",
    "                img = cv2.imread(imgPath,cv2.IMREAD_GRAYSCALE)\n",
    "                images.append(img)\n",
    "#                 print(imgPath)\n",
    "#                 print(img.shape)\n",
    "                labels.append(label)\n",
    "            label += 1\n",
    "    images = np.asarray(images)\n",
    "    labels = np.asarray(labels)   \n",
    "    return images,labels,names     \n",
    "\n",
    "def FaceRec(data):  \n",
    "    # create landmark detector and load lbf model:\n",
    "    facemark = cv2.face.createFacemarkLBF()\n",
    "    facemark.loadModel(\"lbfmodel.yaml\")    \n",
    "    \n",
    "    X,y,names=LoadImages(data)\n",
    "    #create a EigenFaceRecognizer\n",
    "    model = cv2.face.EigenFaceRecognizer_create()\n",
    "     #train the model\n",
    "    model.train(X,y)\n",
    "    print('training is done')\n",
    "    #create a classifier\n",
    "    face_cascade = cv2.CascadeClassifier('D:\\Python37\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml')   \n",
    "   \n",
    "    #open camera   \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow('Camera')    \n",
    "    while(True):\n",
    "        #read a frame\n",
    "        ret,frame = camera.read()\n",
    "        if ret:\n",
    "            #convert into grayscale\n",
    "            grayscale_image = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            #detect faces\n",
    "            faces = face_cascade.detectMultiScale(grayscale_image,1.3,5)   \n",
    "         # (x,y)=top left , w=weight, h=height              \n",
    "        for (x,y,w,h) in faces:\n",
    "            #draw a rectangle contained faces\n",
    "            frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            #create landmarks\n",
    "            ok, landmarks = facemark.fit(frame, faces) \n",
    "            #crop images \n",
    "            grayscale_face = grayscale_image[y:y+h,x:x+w]          \n",
    "            try:\n",
    "                #crop into 250*250 \n",
    "                grayscale_face = cv2.resize(grayscale_face,(250,250),interpolation=cv2.INTER_LINEAR)              \n",
    "                #predict\n",
    "                params = model.predict(grayscale_face)\n",
    "                #print the confidence\n",
    "                print('confidence: ',params[1])\n",
    "                #print landmarks\n",
    "                for person in landmarks:\n",
    "                    for point in person[0]:\n",
    "                        # circle the characteristicsï¼Œtotal of 68\n",
    "                        cv2.circle(frame, (point[0],point[1]), 5, color=(0, 255, 0))\n",
    "                #print the name on the camara\n",
    "                cv2.putText(frame,names[params[0]],(x,y-20),cv2.FONT_HERSHEY_SIMPLEX,1,255,2)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            cv2.imshow('Camera',frame)            \n",
    "            #press q to exit\n",
    "            if cv2.waitKey(100) & 0xff == ord('q') :\n",
    "                break\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "       \n",
    "if __name__=='__main__':\n",
    "    data = 'small_face_dataset'\n",
    "#     generator(data)\n",
    "    FaceRec(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
